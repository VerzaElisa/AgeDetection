{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3759a0bb-6414-4878-aa54-2c914dd66df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 13:33:42.894279: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 13:33:42.955779: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-30 13:33:43.200168: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 13:33:43.200209: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 13:33:43.201521: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 13:33:43.322535: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-30 13:33:43.324564: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 13:33:44.647394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "NUM_CLIENTS = 10\n",
    "ACTIVE_CLIENTS = 5\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49a2724-59c9-4eea-a848-156969190654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "<class 'tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tff_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#def serializable_dataset_fn(client_id):\u001b[39;00m\n\u001b[1;32m     27\u001b[0m      \u001b[38;5;66;03m#   client_data = client_train_dataset[client_id]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m       \u001b[38;5;66;03m#  return tf.data.Dataset.from_tensor_slices(client_data)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[38;5;66;03m#  serializable_dataset_fn=serializable_dataset_fn\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tff_train_data\n\u001b[0;32m---> 38\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_clients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mcreate_clients\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(client_train_dataset[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#def serializable_dataset_fn(client_id):\u001b[39;00m\n\u001b[1;32m     27\u001b[0m  \u001b[38;5;66;03m#   client_data = client_train_dataset[client_id]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m#  return tf.data.Dataset.from_tensor_slices(client_data)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m#  serializable_dataset_fn=serializable_dataset_fn\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtff_train_data\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tff_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('emnist-letters.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size = TEST_SIZE, random_state = 42)\n",
    "\n",
    "def preprocess(dataset):\n",
    "  def batch_format_fn(element):\n",
    "    return (tf.reshape(element[1:], [-1, 28, 28, 1])/255,\n",
    "            tf.reshape(element[0], [-1, 1]))\n",
    "\n",
    "  return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n",
    "\n",
    "def create_clients(dataset):\n",
    "    client_nums = list(range(NUM_CLIENTS))\n",
    "    generator = np.random.default_rng(42)\n",
    "    clients = generator.choice(client_nums, len(dataset))\n",
    "    dataset['client_nums'] = clients\n",
    "    \n",
    "    client_train_dataset = collections.OrderedDict()\n",
    "    grouped_dataset = dataset.groupby('client_nums')\n",
    "    for key, item in grouped_dataset:\n",
    "        current_client = grouped_dataset.get_group(key)\n",
    "        data = collections.OrderedDict((('label',current_client.iloc[:,0]), ('pixels', current_client.iloc[:,1:-1])))\n",
    "        client_train_dataset[key] = data\n",
    "    print(len(client_train_dataset))\n",
    "    print(type(tf.data.Dataset.from_tensor_slices(client_train_dataset[0])))\n",
    "    #def serializable_dataset_fn(client_id):\n",
    "     #   client_data = client_train_dataset[client_id]\n",
    "      #  return tf.data.Dataset.from_tensor_slices(client_data)\n",
    "\n",
    "    #tff_train_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "     #   client_ids=list(client_train_dataset.keys()),\n",
    "      #  serializable_dataset_fn=serializable_dataset_fn\n",
    "    #)\n",
    "    \n",
    "    return tff_train_data\n",
    "    \n",
    "\n",
    "train_df = create_clients(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d704b0f-1751-441f-885d-42a218de2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "  return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Input(shape=(784,)),\n",
    "      tf.keras.layers.Dense(10, kernel_initializer=initializer),\n",
    "      tf.keras.layers.Softmax(),\n",
    "  ])\n",
    "keras_model = create_keras_model()\n",
    "tff_model = tff.learning.models.functional_model_from_keras(\n",
    "    keras_model,\n",
    "    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    input_spec=federated_train_data[0].element_spec,\n",
    "    metrics_constructor=collections.OrderedDict(\n",
    "        accuracy=tf.keras.metrics.SparseCategoricalAccuracy\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25b236-0bca-4fe4-b6bf-fd2137432162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al client update si pu√≤ aggiungere un parametro che indica il numero di epoche in cui ripetere l'addestramento prima di inviare i pesi al server\n",
    "@tf.function\n",
    "def client_update(model, dataset, initial_weights, client_optimizer):\n",
    "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
    "  # Initialize the client model with the current server weights and the optimizer\n",
    "  # state.\n",
    "  client_weights = initial_weights.trainable\n",
    "  optimizer_state = client_optimizer.initialize(\n",
    "      tf.nest.map_structure(tf.TensorSpec.from_tensor, client_weights)\n",
    "  )\n",
    "\n",
    "  # Use the client_optimizer to update the local model.\n",
    "  for batch in dataset:\n",
    "    x, y = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(client_weights)\n",
    "      # Compute a forward pass on the batch of data\n",
    "      outputs = model.predict_on_batch(\n",
    "          model_weights=(client_weights, ()), x=x, training=True\n",
    "      )\n",
    "      loss = model.loss(output=outputs, label=y)\n",
    "\n",
    "    # Compute the corresponding gradient\n",
    "    grads = tape.gradient(loss, client_weights)\n",
    "\n",
    "    # Apply the gradient using a client optimizer.\n",
    "    optimizer_state, client_weights = client_optimizer.next(\n",
    "        optimizer_state, weights=client_weights, gradients=grads\n",
    "    )\n",
    "\n",
    "  return tff.learning.models.ModelWeights(client_weights, non_trainable=())\n",
    "\n",
    "@tf.function\n",
    "def server_update(model, mean_client_weights):\n",
    "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "  del model  # Unused, just take the mean_client_weights.\n",
    "  return mean_client_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91832b-610d-4e4e-a9f3-98c66ddca9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tff.tensorflow.computation\n",
    "def server_init():\n",
    "  return tff.learning.models.ModelWeights(*tff_model.initial_weights)\n",
    "    \n",
    "model_weights_type = server_init.type_signature.result\n",
    "tf_dataset_type = tff.SequenceType(tff.types.tensorflow_to_type(tff_model.input_spec))\n",
    "\n",
    "@tff.tensorflow.computation(tf_dataset_type, model_weights_type)\n",
    "def client_update_fn(tf_dataset, server_weights):\n",
    "  client_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
    "  return client_update(tff_model, tf_dataset, server_weights, client_optimizer)\n",
    "\n",
    "@tff.tensorflow.computation(model_weights_type)\n",
    "def server_update_fn(mean_client_weights):\n",
    "  return server_update(tff_model, mean_client_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49222d-2f06-44ad-bcde-7b45679d2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
    "\n",
    "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
    "def next_fn(server_weights, federated_dataset):\n",
    "  # Broadcast the server weights to the clients.\n",
    "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "  # Each client computes their updated weights.\n",
    "  client_weights = tff.federated_map(\n",
    "      client_update_fn, (federated_dataset, server_weights_at_client)\n",
    "  )\n",
    "\n",
    "  # The server averages these updates.\n",
    "  mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "  # The server updates its model.\n",
    "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "\n",
    "  return server_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbebefe-c036-4d58-862b-2f146a52aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation\n",
    "def initialize_fn():\n",
    "  return tff.federated_eval(server_init, tff.SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d45946e2-dfab-40e5-b03c-5222ff0721c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_emnist_test = emnist_test.create_tf_dataset_from_all_clients()\n",
    "central_emnist_test = preprocess(central_emnist_test)\n",
    "\n",
    "def evaluate(model_weights):\n",
    "  keras_model = create_keras_model()\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "  )\n",
    "  model_weights.assign_weights_to(keras_model)\n",
    "  keras_model.evaluate(central_emnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2953253e-27d4-45de-8f4d-14516409eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2042/2042 [==============================] - 5s 2ms/step - loss: 2.7959 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 4s 2ms/step - loss: 2.7273 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.7292 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.7140 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.7029 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6909 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6792 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6675 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6559 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6442 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 5s 3ms/step - loss: 2.6326 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6211 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.6096 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.5981 - sparse_categorical_accuracy: 0.0980\n",
      "2042/2042 [==============================] - 6s 3ms/step - loss: 2.5867 - sparse_categorical_accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "federated_algorithm = tff.templates.IterativeProcess(initialize_fn=initialize_fn, next_fn=next_fn)\n",
    "server_state = federated_algorithm.initialize()\n",
    "for _ in range(15):\n",
    "  server_state = federated_algorithm.next(server_state, federated_train_data)\n",
    "  evaluate(server_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df8697-8134-4e9d-8775-b40f92cdc982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
